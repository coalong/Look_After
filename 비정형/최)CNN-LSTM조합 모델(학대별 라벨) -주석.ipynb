{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"최)CNN-LSTM조합 모델(학대별 라벨) -주석.ipynb","provenance":[{"file_id":"1Lvf9Lp0uXiaYoPNmuJ_QAGQNjyWqGvZo","timestamp":1605796084425},{"file_id":"1klXyFdP02-Z-MCU7Ginq0M_BahWJaTrn","timestamp":1601702406208},{"file_id":"1T3CHrlvKdqPw5lNFg92DC5TrBf_PWAzk","timestamp":1601013955184}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-8Z9HAxVLZan"},"source":["필요한 라이브러리 설치"]},{"cell_type":"code","metadata":{"id":"tq0DFkoQxZ3V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614247269211,"user_tz":-540,"elapsed":14042,"user":{"displayName":"이아리","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_9zOjRuxnm5cXYUnTwVgTBNla-W9ycSZfRL03=s64","userId":"11816708880707676333"}},"outputId":"de0a8e75-429d-456b-cd67-014bdc112129"},"source":["!pip install tensorflow==2.2.0 #tensorflow 2.2.0 설치"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorflow==2.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/1a/0d79814736cfecc825ab8094b39648cc9c46af7af1bae839928acb73b4dd/tensorflow-2.2.0-cp37-cp37m-manylinux2010_x86_64.whl (516.2MB)\n","\u001b[K\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4rlVLSFfdzSz"},"source":["import tensorflow as tf #tensorflow 버전 확인\n","tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nUGFk8-m81fn"},"source":["#필요한 모듈 불러오기\n","\n","import re\n","import nltk\n","\n","import pandas as pd\n","import numpy as np\n","\n","\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","english_stemmer=nltk.stem.SnowballStemmer('english')\n","\n","from sklearn.feature_selection.univariate_selection import SelectKBest, chi2, f_classif\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import SGDClassifier, SGDRegressor\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import random\n","import itertools\n","\n","import sys\n","import os\n","import argparse\n","from sklearn.pipeline import Pipeline\n","from scipy.sparse import csr_matrix\n","from sklearn.feature_extraction.text import CountVectorizer\n","import six\n","import tensorflow\n","from abc import ABCMeta\n","from scipy import sparse\n","from scipy.sparse import issparse\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils import check_X_y, check_array\n","from sklearn.utils.extmath import safe_sparse_dot\n","from sklearn.preprocessing import normalize, binarize, LabelBinarizer\n","from sklearn.svm import LinearSVC\n","\n","from keras.preprocessing import sequence\n","from keras.utils import np_utils\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Activation, Lambda\n","from keras.layers.embeddings import Embedding\n","from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n","from keras.preprocessing.text import Tokenizer\n","from collections import defaultdict\n","from keras.layers.convolutional import Convolution1D\n","from keras import backend as K\n","\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib import cm\n","%matplotlib inline\n","from matplotlib import font_manager, rc\n","font_location = 'C:\\Windows\\Fonts\\malgunsl.ttf'\n","plt.rcParams['axes.unicode_minus'] = False\n","import sklearn.metrics as metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ic-MYhOke0xu"},"source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt\n"," \n","%config InlineBackend.figure_format = 'retina'\n"," \n","!apt -qq -y install fonts-nanum\n"," \n","import matplotlib.font_manager as fm\n","fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n","font = fm.FontProperties(fname=fontpath, size=9)\n","plt.rc('font', family='NanumBarunGothic') \n","fm._rebuild()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lo6WBlg8esSR"},"source":["\\# import pandas as pd\n","# import numpy as np\n","# import re\n","\n","# # 그래프에서 한글표현을 위해 폰트를 설치합니다.\n","# %config InlineBackend.figure_format = 'retina'\n","\n","# !apt -qq -y install fonts-nanum > /dev/null\n","# import matplotlib.font_manager as fm\n","# fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n","# font = fm.FontProperties(fname=fontpath, size=9)\n","\n","# from plotnine import *\n","# import plotnine\n","\n","# # 기본 글꼴 변경\n","# import matplotlib as mpl\n","# mpl.font_manager._rebuild()\n","# mpl.pyplot.rc('font', family='NanumBarunGothic')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrXaysV3jj7S"},"source":["!sudo apt-get install -y fonts-nanum\n","#폰트 리스트 갱신\n","!sudo fc-cache -fv\n","!rm ~/.cache/matplotlib -rf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6oGowDUniH3M"},"source":["import matplotlib.pyplot as plt\n","import matplotlib.animation as anim\n","import matplotlib as mpl\n","import matplotlib.font_manager as fm\n","\n","#\n","def plot_cont(fun, xmax):\n","    y = []\n","    fig = plt.figure()\n","    ax = fig.add_subplot(1,1,1)\n","\n","    def update(i):\n","        yi = fun()\n","        y.append(yi)\n","        x = range(len(y))\n","        ax.clear()\n","        ax.plot(x, y)\n","\n","    print ( i, ': ', yi)\n","\n","    a = anim.FuncAnimation(fig, update, frames=xmax, repeat=False)\n","    plt.show()\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cJQx0AbavKBE"},"source":["import matplotlib as mpl\n","import matplotlib.pyplot as plt\n"," \n","# %config InlineBackend.figure_format = 'retina'\n"," \n","# !apt -qq -y install fonts-nanum\n"," \n","import matplotlib.font_manager as fm\n","\n","fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n","font = fm.FontProperties(fname=fontpath, size=9)\n","plt.rc('font', family='NanumBarunGothic') \n","# mpl.font_manager._rebuild()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dIAsj19DLZao"},"source":["형태소 분석기 설치"]},{"cell_type":"code","metadata":{"id":"xNEntKOZ88K7"},"source":["# 형태소분석기 관련 설치\n","!apt-get update\n","!apt-get install g++ openjdk-8-jdk\n","!pip install JPype1==0.7.4\n","!pip install rhinoMorph"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lAFuDRtl9Ooe"},"source":["import rhinoMorph\n","\n","rn = rhinoMorph.startRhino()\n","text = \"한글로 된 한글텍스트를 분석하는 것은 즐겁다.\"\n","\n","# 사용 1 : 모든 형태소 보이기\n","text_analyzed = rhinoMorph.onlyMorph_list(rn, text)\n","print('\\n1. 형태소 분석 결과: ', text_analyzed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y3D0C8Cm9Vqz"},"source":["# import rhinoMorph\n","\n","# rn = rhinoMorph.startRhino()\n","# text = \"한글로 된 한글텍스트를 분석하는 것은 즐겁다.\"\n","\n","# # 사용 1 : 모든 형태소 보이기\n","# text_analyzed = rhinoMorph.onlyMorph_list(rn, text)\n","# print('\\n1. 형태소 분석 결과: ', text_analyzed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nOJPS2WJ9a_e"},"source":["java_home = \"C:\\Program Files\\Java\\jdk-14.0.2\" "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXs0rh2F9fTO"},"source":["# 구글 드라이브와 연결하기\n","# from google.colab import auth\n","# auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aJRWby279lle"},"source":["!ls \"/content/gdrive/My Drive/\"  #현재 경로 파일 보기"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhxhTIKB9ooo"},"source":["# 경로 변경\n","#!cd /content/gdrive/My\\ Drive/pytest/\n","%cd /content/gdrive/My\\ Drive/pytest/\n","#!pip install -U -q PyDrive"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bfFujiXk9qyy"},"source":["# # 데이터 로딩\n","# def read_data(filename, encoding='utf-8'):                # 읽기 함수 정의\n","#   with open(filename, 'r', encoding=encoding) as f:\n","#     data = [line.split('\\t') for line in f.read().splitlines()]\n","#     data = data[1:]                 # txt 파일의 헤더(id document label)는 제외하기\n","#   return data\n","\n","# def write_data(data, filename, encoding='utf-8'):         # 쓰기 함수도 정의\n","#   with open(filename, 'w', encoding=encoding) as f:\n","#     f.write(data)\n","\n","# #data = read_data('/content/gdrive/My Drive/pytest/ratings_small.txt', encoding='cp949')\n","# data = read_data('학대별 라벨.csv', encoding='utf-8')  # 전체파일은 ratings.txt (긍정 1만, 부정 1만)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OwcUULgOLZao"},"source":["데이터 불러오기"]},{"cell_type":"code","metadata":{"id":"Ae1WxHIs9tQ9"},"source":["# 데이터 로딩\n","df1 = pd.read_csv('./학대별 라벨.csv', encoding = 'utf-8')\n","df1\n","#sample :랜덤 샐플 추출 \n","# 1 : 정석학대\n","# 2 : 신체학대\n","# 3 : 방임\n","# 4 : 성학대 "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjIchX_v9vxg"},"source":["#번호컬럼 제거\n","\n","df1 = df1.drop(['번호'], axis = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_b5ix1Z89y4K"},"source":["#전체 값 리스트(2차원 리스트로 출력)\n","list1 = df1.values.tolist()\n","\n","list1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbYpzgml91xF"},"source":["df1['Label'].value_counts() #학대 유형별 데이터셋 수"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2B_vmdmT96vo"},"source":["#문자열 제외 제거 \n","def text_cleaning(doc):\n","    \n","    doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\", str(doc))\n","        \n","    return doc\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SM1J_lt699Xh"},"source":["def define_stopwords(path):\n","    \n","    SW = set() #SW는 불용어 사전\n","    #집합형태로 만들어줘야 중복을 제외하고 출력해줌\n","    #불용어들을 추가할려면 SW.add()이렇게 넣어주면 됨\n","    \n","    with open(path, encoding = 'cp949') as f:\n","        for word in f:\n","            SW.add(word)\n","            \n","    return SW\n","\n","#with open을 통해 파일을 열고 해당 파일에 있는 단어들을 SW에 넣어줌 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WEHlTWtbLZap"},"source":["형태소 분석"]},{"cell_type":"code","metadata":{"id":"d2zhJ-xc-AMa"},"source":["#텍스트 토큰화\n","def text_tokenizing(doc):\n","    return [word for word in rhinoMorph.onlyMorph_list(rn,doc, pos = ['NNG', 'NNP','NP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi = False) if word not in a and len(word) > 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BdZ7m6w7LZap"},"source":["불용언 처리"]},{"cell_type":"code","metadata":{"id":"KM_IeC8H-DI5"},"source":["SW = define_stopwords('./stopwords-ko.txt')  #빈도수 분석을 통해 만든 불용어사전(text파일)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Dgb1oxrqHvp"},"source":["SW.add('카레')\n","SW.add('엄마')\n","SW.add('아빠') #불용언에 엄마, 아빠, 카레 단어 추가"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmjmukIA-Ex4"},"source":["a = [] \n","for i in SW:\n","    a.append(i.replace('\\n', ''))  #SW에있는 단어 띄어쓰기 제어하고 배열a에 다시 저장"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ui152sMT-Grf"},"source":["# a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHh8TNaF-Iaa"},"source":["#오름차순 정렬\n","SW = set(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lUeRCIO0-NBK"},"source":["len(SW) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dG0f0VoK-Osu"},"source":["!pip install konlpy\n","from konlpy.tag import Mecab\n","import json\n","import os\n","import re\n","from pprint import pprint"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uPYl3nf5-Sha"},"source":["#텍스트 클리닝과 텍스트 토큰화\n","from konlpy.tag import Okt\n","from konlpy.tag import Kkma\n","from konlpy.tag import Hannanum\n","#from konlpy.tag import Okt 보통의 경우에는 Okt를 사용하지 x\n","import json\n","import os\n","import re\n","from pprint import pprint\n","\n","# okt = Okt()\n","# mecab = Mecab()\n","# kkma = Kkma()\n","# hannanum = Hannanum()\n","\n","SW = define_stopwords('./stopwords-ko.txt') # 불용어들을 SW에 저장 \n","\n","if os.path.exists('Multi1_train_docs.json'): #op.path.exists() 어떤 파일이 디렉토리에 있는지 확인하는 함수 있으면 true출력\n","    with open(\"Multi1_train_docs.json\", encoding='utf-8') as f: #if를 만족하면 파일을 불러올 수 있다 \n","        train_data = json.load(f)\n","else:\n","    #토큰화와 클리닝까지 \n","    #투플은 리스트와 비슷하지만 값은 변경할 수 없다 \n","    #line[1]을 토큰화하는 이유는 line[1]에 리뷰가 존재\n","    #클리닝을 먼저하고 토큰화를 하는게 좋다 \n","    train_data = [(text_tokenizing(text_cleaning(line[0])), line[1])for line in list1 if text_tokenizing(text_cleaning(line[0]))]\n","    #클리닝을 넣고 토큰화진행  \n","    #이게 토큰화만 수행하는 코드 train_data = [(text_tokenizing(line[1]), line[2]) for line in train_docs if text_tokenizing(line[1])] if뒤에는 만약 ㅋㅋㅋ같은거는 토큰화하면 날라가므로 그러면 거기에는 빈 리스트만 존재하니깐 그걸 방지하기 위해\n","    \n","    with open(\"Multi1_train_docs.json\", 'w', encoding='utf-8') as f:\n","        json.dump(train_data, f, ensure_ascii=False, indent='\\t')\n","        \n","# if os.path.exists('test_docs.json'):\n","#     with open(\"test_docs.json\", encoding='utf-8') as f:\n","#         test_data = json.load(f)\n","# else:\n","#     test_data = [(text_tokenizing(text_cleaning(line[1])), line[2]) for line in test_docs if text_tokenizing(text_cleaning(line[1]))]\n","#     #test_data = [(text_tokenizing(line[1]), line[2]) for line in test_docs if text_tokenizing(line[1])]\n","#     with open(\"test_docs.json\", 'w', encoding='utf-8') as f:\n","#         json.dump(test_data, f, ensure_ascii=False, indent='\\t')\n","\n","# pprint(train_data[0])\n","# pprint(test_data[0])\n","import nltk\n","total_tokens = [token for doc in train_data for token in doc]\n","len(total_tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1pLmoZHtLZap"},"source":["Label dataframe만들기"]},{"cell_type":"code","metadata":{"id":"WIyE_DGV-W3F"},"source":["Senti = []    #라벨 1,2,3,4를 배열Senti에 넣기\n","for i in range(len(train_data)):\n","    Senti.append(train_data[i][1]) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S24aRHI3-ZBp"},"source":["#row수에 맞게 컬럼에 라벨값 넣기.\n","Senti = pd.DataFrame(Senti)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YeXzvO6r-epM"},"source":["Senti"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IZKNRd9s-gK1"},"source":["#컬럼이름 0->Label로 변경\n","Senti.rename(columns = { 0 : 'Label'}, inplace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Bk2mfdw-hhn"},"source":["Senti"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ngq2IY6Q-jHG"},"source":["#label unique확인\n","labels = Senti['Label']\n","\n","classes = sorted(labels.unique())\n","classes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGt8BbUn-k1F"},"source":["#라벨 1,2,3,4에 대해서 더미화 하기\n","label_to_cat = dict()\n","for i in range(len(classes)):\n","    dummy = np.zeros((len(classes),), dtype = 'int8')\n","    dummy[i] = 1\n","    label_to_cat[classes[i]] = dummy\n","    \n","label_to_cat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfXWpyqN-m9p"},"source":["#numpy array로 바꿔 행렬 만들기\n","y = np.array([label_to_cat[label] for label in Senti.Label])\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZMknn9UG-rgB"},"source":["#데이터 본문 배열 Text에 넣기\n","Text = []\n","for i in range(len(train_data)):    \n","    Text.append(train_data[i][0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaOUsTLD-uq9"},"source":["print(len(y))\n","print(len(Text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8n3NzMi-xTi"},"source":["max_words = 5000 #데이터셋에서 가장 빈도 높은 10,000개의 단어만 사용\n","max_len = 100 # 사용할 텍스트의 길이(가장 빈번한 max_features개의 단어만 사용)\n","Batch_size = 32\n","Epochs = 20"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cQXn3FEe-_aP"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import numpy as np\n","import math\n","import tensorflow as tf\n","from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from keras.layers import Activation, SimpleRNN\n","import gensim\n","from gensim import models\n","from gensim.models import Word2Vec, KeyedVectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nGgB_oZBLZap"},"source":["Embedding"]},{"cell_type":"code","metadata":{"id":"SrD2tPOP_KJa"},"source":["#단어의 유사도 벡터화\n","embeddings = Word2Vec(size=200, min_count=3, window = 3) #최소빈도 3이상/ 주변단어 포함 범위\n","embeddings.build_vocab([sentence for sentence in Text])\n","embeddings.train([sentence for sentence in Text],\n","                 total_examples=embeddings.corpus_count,\n","                 epochs=embeddings.epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wJd_3Vl_MxI"},"source":["embeddings.save('Multi1_Embedding_유형별.model')\n","print('완료')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QpDk3jRcLZap"},"source":["CBOW로 시각화 하기"]},{"cell_type":"code","metadata":{"id":"NHBOwVx6_Q8M"},"source":["#'상처'와 유사한 단어의 벡터값 보여주기 \n","vocab = dict(embeddings.wv.most_similar('상처'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"829dl-VS_Td5"},"source":["X = embeddings[vocab]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-No-EoYl_U6Q"},"source":["#시각화 알고리즘 사용하기\n","from sklearn.manifold import TSNE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ycM8PVf4_WpV"},"source":["tsne = TSNE(n_components = 2)\n","X_tsne = tsne.fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VtPOFr1c_Y4M"},"source":["#'상처'유사 단어 벡터 X,Y값\n","df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m0hj6UKh_0v6"},"source":["#그래프기리기\n","fig = plt.figure()\n","fig.set_size_inches(5, 5)\n","ax = fig.add_subplot(1, 1, 1)\n","\n","ax.scatter(df['x'], df['y'])\n","\n","for word, pos in df.iterrows():\n","    ax.annotate(word, pos,fontsize=15)\n","    \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4O9nmxL_5Az"},"source":["def encode_sentence_lstm(tokens, emb_size):\n","    #80X200크기의 행열을 튜플로 전달\n","    vec = np.zeros((80, 200))\n","    \n","    for i, word in enumerate(tokens):\n","        if i > 79:\n","            break\n","        try:\n","            vec[i] = embeddings.wv[word].reshape((1, emb_size))\n","        except KeyError:\n","            continue\n","    return vec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xMkD8Bv2LZap"},"source":["train / test set 만들기"]},{"cell_type":"code","metadata":{"id":"ViwDA6Su_7M5"},"source":["#train / validation 분리\n","X = np.array([encode_sentence_lstm(ele, 200) for ele in map(lambda x: x, Text)])\n","\n","train_text, test_text, train_senti, test_senti = train_test_split(X, y, stratify = y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MM9eqLNrGE2X"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0DzC247_-vc"},"source":["train_text.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9jQiT7etAAt9"},"source":["len(train_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUxOX2g3AC8p"},"source":["len(train_senti)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CztYM3l3AIao"},"source":["\n","import keras.utils\n","from keras.layers import Dense, Activation, Flatten\n","from tensorflow.keras import models\n","from tensorflow.keras import layers\n","from keras.layers import Input, Dense, LSTM, GRU, LeakyReLU, Dropout,Conv1D,MaxPooling1D\n","from keras.engine import Layer\n","from keras.models import Sequential, Model, load_model\n","from keras import backend as K\n","from keras.engine import Layer\n","from keras.models import Sequential, Model, load_model\n","from keras.layers import Input, Dense, LSTM, GRU, LeakyReLU, Dropout\n","from keras.layers import  Embedding, Bidirectional\n","from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n","from keras.optimizers import Adam\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4s2LUeGlLZap"},"source":["CNN-LSTM 조합 모델"]},{"cell_type":"code","metadata":{"id":"5JQm_DSHATpa"},"source":["# create the model\n","#CNN - LSTM 조합 모델\n","input_tensor = Input(shape=(80,200))\n","embedding_vecor_length = 32\n","model = Sequential()\n","x = Conv1D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu')(input_tensor)\n","x = MaxPooling1D(pool_size = 2)(x)\n","x = Bidirectional(LSTM(256, return_sequences = True, dropout = 0.2))(x)\n","x = Bidirectional(LSTM(128, return_sequences = False))(x)\n","x = Dense(128, activation = 'relu')(x)\n","x = Dropout(0.3)(x)\n","x = Dense(64, activation = 'relu')(x)\n","x = Dense(16, activation='relu')(x)\n","\n","\n","output_tensor = Dense(4, activation = 'softmax')(x)\n","\n","model = Model(inputs = [input_tensor], outputs = [output_tensor])\n","model.summary()\n","\n","from IPython.display import SVG\n","from keras.utils import model_to_dot\n","\n","\n","SVG(model_to_dot(model, dpi=65).create(prog='dot', format='svg'))\n","# model.add(Embedding(input_dim= max_words,input_length = max_len))\n","# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n","# model.add(MaxPooling1D(pool_size=2))\n","\n","# model.add(LSTM(256, dropout = 0.2))\n","\n","# model.add(Dense(1, activation='sigmoid'))\n","\n","\n","# model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"We7RWYVIV-gY"},"source":["#patience = 성능이 증가하지 않는 epoch을 몇 번이나 허용\n","#verbose = 언제 keras에서 training을 멈추었는지 화면에 출력\n","earlystopper = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = , verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lh3KFHkGWjR1"},"source":["#다중분류 categorical_accuracy() -> 정확도 계산\n","model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjEFhEqiLZap"},"source":["model.fit (수정하면서 결과보기)"]},{"cell_type":"code","metadata":{"id":"RsO4oKm1WnVc"},"source":["history = model.fit(train_text, train_senti, epochs=50, verbose=1, batch_size = 32, validation_data = (test_text, test_senti), callbacks = [earlystopper])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHPLmrO-5Wy9"},"source":["\n","print(test_text.shape)\n","print(len(test_senti))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR8BB7t55YWg"},"source":["model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ka5s5TRuLZap"},"source":["test acc 결과보기"]},{"cell_type":"code","metadata":{"id":"VbLeK6Qe61zZ"},"source":["#test accracy보기\n","score, acc = model.evaluate(test_text, test_senti, verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXLEa2nPTbQ0"},"source":["print('Test Score:' , score)\n","print('Test Accuracy:' , acc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NKSvlUZtTeBn"},"source":["import matplotlib.pyplot as plt\n","from math import pi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tPN-ZNvhdvgs"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GpwGNOFYTfaJ"},"source":["pred_probs = model.predict(train_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CO79-GqPTh9y"},"source":["print(np.round(pred_probs[:10],3))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uTz_C61hLZaq"},"source":["Val_loss 그래프로 확인"]},{"cell_type":"code","metadata":{"id":"3cVFHZRNTkwx"},"source":["#그래프 설정\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","plt.figure()\n","plt.plot(loss, 'ro-', label = 'train_loss')\n","plt.plot(val_loss, 'bo-', label = 'val_loss')\n","plt.ylabel('Cross Entrophy')\n","plt.xlabel('Epoch')\n","plt.legend(loc = 'upper right')\n","plt.title('Training and Validation Loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8F2vhaL0LZaq"},"source":["test 결과 레이더 차트로 보여주기"]},{"cell_type":"code","metadata":{"id":"sIC5dY9JYp57"},"source":["#test 결과 레이더 차트로 보여주기\n","def sentiment_predict(new_sentence):\n","    if new_sentence != '':\n","        new_sentence1 = text_cleaning(new_sentence)\n","        new_sentence2 = rhinoMorph.onlyMorph_list(rn,new_sentence1, pos = ['NNG', 'NNP','NP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'], eomi = False)\n","        new_sentence3 = [word for word in new_sentence2 if not word in SW] # 불용어 제거\n","        if new_sentence3 != []:\n","            X = np.array([encode_sentence_lstm(new_sentence3,200)])\n","            pred_probs = model.predict(X) # 예측\n","            \n","            categories = ['정서학대', '신체학대','방임', '성학대']\n","\n","            N = len(categories)\n","\n","            values = np.round(pred_probs, 3).flatten().tolist()\n","            values += values[:1]\n","\n","            angles = [n / float(N) * 2 * pi for n in range(N)]\n","            angles += angles[:1]\n","\n","            plt.polar(angles, values)\n","            plt.fill(angles, values, alpha = 0.5)\n","            plt.xticks(angles[:-1], categories)\n","\n","            plt.yticks([0,0.5, 1.0])\n","            plt.ylim(0,1)\n","            \n","            plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KvuCAxg1LZaq"},"source":["검증해보기"]},{"cell_type":"code","metadata":{"id":"ZiB4tW2VUIr8"},"source":["#예시 적어보기\n","\n","doc = input()\n","#엄마가 내몸을 만지고 아빠가 나를 때렸어요."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJnO7hO-YsdS"},"source":["#예측\n","\n","sentiment_predict(doc)\n","#아동은 혼자 집에 방치되어 있었다. 라면만 쌓여있고 부엌은 어질러져 있었다. 부모는 매일 술을 마시고 새벽 늦게 들어오는 것으로 확인된다. 부모는 술을 마시면 아동에게 습관적으로 욕을 함."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnMB9T_Ko95o"},"source":["sentiment_predict(doc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeeOpf-4LZaq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MDP5Nl3RLZaq"},"source":["Model 저장"]},{"cell_type":"code","metadata":{"id":"jT6wZVETddPe"},"source":["#모델 jason으로 저장\n","model_json = model.to_json()\n","with open('./Multi_model2.json', 'w') as json_file:\n","    json_file.write(model_json)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hDIzl-6Y4VL"},"source":["#모델 가중치 저장\n","model.save_weights('./Multi_weight2.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmBEf1TZ5zaM"},"source":[""],"execution_count":null,"outputs":[]}]}